#!/usr/bin/env python3
"""
Script para verificar detecci√≥n de GPU NVIDIA
"""


def check_gpu_status():
    """Verifica el estado completo de la GPU"""

    print("üîç VERIFICANDO ESTADO DE GPU")
    print("=" * 50)

    # 1. Verificar NVIDIA drivers
    print("\n1Ô∏è‚É£ NVIDIA Drivers:")
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ nvidia-smi funciona")
            # Extraer info b√°sica
            lines = result.stdout.split('\n')
            for line in lines:
                if 'RTX' in line or 'GTX' in line:
                    print(f"   üéÆ GPU encontrada: {line.strip()}")
                elif 'CUDA Version' in line:
                    cuda_version = line.split('CUDA Version: ')[1].split()[0]
                    print(f"   üîß CUDA Driver Version: {cuda_version}")
        else:
            print("‚ùå nvidia-smi no funciona")
            print("   Instala los drivers NVIDIA")
    except FileNotFoundError:
        print("‚ùå nvidia-smi no encontrado")
        print("   Instala los drivers NVIDIA")

    # 2. Verificar PyTorch
    print("\n2Ô∏è‚É£ PyTorch:")
    try:
        import torch
        print(f"‚úÖ PyTorch instalado: {torch.__version__}")
        print(f"   üîß CUDA disponible: {torch.cuda.is_available()}")

        if torch.cuda.is_available():
            print(f"   üéÆ Dispositivos CUDA: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"   üìä GPU {i}: {props.name}")
                print(f"   üíæ VRAM: {props.total_memory / 1e9:.1f} GB")
                print(f"   üî¢ CUDA Capability: {props.major}.{props.minor}")
        else:
            print("   ‚ùå CUDA no disponible en PyTorch")
            print(f"   üîß PyTorch compilado con CUDA: {torch.version.cuda}")
            if torch.version.cuda is None:
                print("   ‚ö†Ô∏è PyTorch instalado SIN soporte CUDA")

    except ImportError:
        print("‚ùå PyTorch no instalado")

    # 3. Verificar sentence-transformers
    print("\n3Ô∏è‚É£ Sentence Transformers:")
    try:
        from sentence_transformers import SentenceTransformer
        print("‚úÖ sentence-transformers instalado")

        # Probar cargar modelo peque√±o en GPU
        try:
            model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')
            print("‚úÖ Modelo cargado en GPU exitosamente")

            # Test r√°pido
            test_text = ["Hello world"]
            embedding = model.encode(test_text)
            print(f"‚úÖ Test embedding: {embedding.shape}")

        except Exception as e:
            print(f"‚ùå Error cargando modelo en GPU: {e}")

    except ImportError:
        print("‚ùå sentence-transformers no instalado")

    # 4. Recomendaciones
    print("\nüí° RECOMENDACIONES:")
    print("=" * 50)

    try:
        import torch
        if not torch.cuda.is_available():
            print("üîß Reinstalar PyTorch con CUDA:")
            print("   pip uninstall torch torchvision torchaudio")
            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    except ImportError:
        print("üîß Instalar PyTorch con CUDA:")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")


def test_embedding_performance():
    """Test de rendimiento para comparar CPU vs GPU"""
    try:
        import torch
        from sentence_transformers import SentenceTransformer
        import time

        if not torch.cuda.is_available():
            print("‚ùå CUDA no disponible, saltando test de rendimiento")
            return

        print("\nüèÉ‚Äç‚ôÇÔ∏è TEST DE RENDIMIENTO")
        print("=" * 50)

        # Datos de prueba
        test_texts = [f"This is test sentence number {i}" for i in range(100)]

        # Test CPU
        print("\nüêå Probando en CPU...")
        model_cpu = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')
        start_time = time.time()
        embeddings_cpu = model_cpu.encode(test_texts, batch_size=32)
        cpu_time = time.time() - start_time
        print(f"   ‚è±Ô∏è Tiempo CPU: {cpu_time:.2f} segundos")

        # Test GPU
        print("\nüöÄ Probando en GPU...")
        model_gpu = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')
        start_time = time.time()
        embeddings_gpu = model_gpu.encode(test_texts, batch_size=64)
        gpu_time = time.time() - start_time
        print(f"   ‚è±Ô∏è Tiempo GPU: {gpu_time:.2f} segundos")

        # Comparaci√≥n
        speedup = cpu_time / gpu_time
        print(f"\nüèÜ Aceleraci√≥n GPU: {speedup:.1f}x m√°s r√°pido")

        if speedup > 2:
            print("‚úÖ GPU funcionando correctamente")
        else:
            print("‚ö†Ô∏è GPU no est√° dando la aceleraci√≥n esperada")

    except Exception as e:
        print(f"‚ùå Error en test de rendimiento: {e}")


if __name__ == "__main__":
    check_gpu_status()
    test_embedding_performance()